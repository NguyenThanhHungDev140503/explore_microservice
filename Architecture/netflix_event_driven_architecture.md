# Netflix Event-Driven Architecture - PhÃ¢n TÃ­ch Chi Tiáº¿t

## Má»¥c Lá»¥c

1. [Tá»•ng Quan](#1-tá»•ng-quan)
2. [CÃ¡c ThÃ nh Pháº§n ChÃ­nh](#2-cÃ¡c-thÃ nh-pháº§n-chÃ­nh)
3. [Keystone Pipeline](#3-keystone-pipeline)
4. [Real-Time Distributed Graph (RDG)](#4-real-time-distributed-graph-rdg)
5. [Kafka - Backbone cá»§a Event Streaming](#5-kafka---backbone-cá»§a-event-streaming)
6. [Apache Flink - Real-Time Processing](#6-apache-flink---real-time-processing)
7. [Ads Event Processing Pipeline](#7-ads-event-processing-pipeline)
8. [Best Practices vÃ  Lessons Learned](#8-best-practices-vÃ -lessons-learned)
9. [So SÃ¡nh vá»›i CÃ¡c Giáº£i PhÃ¡p KhÃ¡c](#9-so-sÃ¡nh-vá»›i-cÃ¡c-giáº£i-phÃ¡p-khÃ¡c)
10. [Káº¿t Luáº­n](#10-káº¿t-luáº­n)

---

## 1. Tá»•ng Quan

### 1.1 Event-Driven Architecture lÃ  gÃ¬?

**Event-Driven Architecture (EDA)** lÃ  má»™t mÃ´ hÃ¬nh kiáº¿n trÃºc pháº§n má»m trong Ä‘Ã³ cÃ¡c thÃ nh pháº§n giao tiáº¿p vá»›i nhau thÃ´ng qua viá»‡c táº¡o ra, phÃ¡t hiá»‡n vÃ  xá»­ lÃ½ cÃ¡c **events** (sá»± kiá»‡n). Thay vÃ¬ gá»i trá»±c tiáº¿p cÃ¡c service khÃ¡c (synchronous), cÃ¡c service sáº½ phÃ¡t ra events vÃ  cÃ¡c service quan tÃ¢m sáº½ láº¯ng nghe vÃ  pháº£n á»©ng vá»›i nhá»¯ng events Ä‘Ã³ (asynchronous).

### 1.2 Táº¡i sao Netflix chá»n Event-Driven Architecture?

Netflix xá»­ lÃ½ má»™t khá»‘i lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ vá»›i:

| Thá»‘ng KÃª | Sá»‘ Liá»‡u |
|----------|---------|
| Subscribers | 260+ triá»‡u ngÆ°á»i dÃ¹ng |
| Giá» xem trung bÃ¬nh/ngÃ y | 3.2 giá»/ngÆ°á»i |
| Events xá»­ lÃ½ | HÃ ng chá»¥c triá»‡u events/giÃ¢y |
| Phim vÃ  series | 5,000+ phim, 50,000+ táº­p |

**LÃ½ do chÃ­nh:**

- **Scalability**: Xá»­ lÃ½ hÃ ng triá»‡u events/giÃ¢y
- **Decoupling**: CÃ¡c microservices Ä‘á»™c láº­p, dá»… triá»ƒn khai
- **Resilience**: Há»‡ thá»‘ng chá»‹u lá»—i tá»‘t hÆ¡n
- **Real-time Processing**: PhÃ¢n tÃ­ch vÃ  pháº£n há»“i real-time
- **Flexibility**: Dá»… dÃ ng thÃªm consumers má»›i

### 1.3 Kiáº¿n TrÃºc Tá»•ng Quan

```mermaid
flowchart TB
    subgraph Client["ğŸ“± Client Devices"]
        Mobile["Mobile App"]
        Web["Web App"]
        TV["Smart TV"]
    end
    
    subgraph Gateway["ğŸŒ API Gateway Layer"]
        Zuul["Netflix Zuul"]
    end
    
    subgraph EventBus["ğŸ“¨ Event Bus Layer"]
        Kafka["Apache Kafka Clusters"]
    end
    
    subgraph Processing["âš¡ Stream Processing Layer"]
        Flink["Apache Flink Jobs"]
        Keystone["Keystone Pipeline"]
    end
    
    subgraph Storage["ğŸ’¾ Storage Layer"]
        Cassandra["Cassandra"]
        S3["AWS S3"]
        Iceberg["Apache Iceberg"]
        EVCache["EVCache"]
    end
    
    subgraph Analytics["ğŸ“Š Analytics Layer"]
        Spark["Apache Spark"]
        Druid["Apache Druid"]
        Presto["Presto"]
    end
    
    Client --> Gateway
    Gateway --> EventBus
    EventBus --> Processing
    Processing --> Storage
    Storage --> Analytics
    
    style EventBus fill:#ff6b6b,color:#fff
    style Processing fill:#4ecdc4,color:#fff
```

---

## 2. CÃ¡c ThÃ nh Pháº§n ChÃ­nh

### 2.1 Event Producers (Nguá»“n phÃ¡t sinh Events)

Events trong Netflix Ä‘Æ°á»£c sinh ra tá»« nhiá»u nguá»“n khÃ¡c nhau:

```mermaid
flowchart LR
    subgraph Producers["Event Producers"]
        UI["UI Interactions"]
        Playback["Playback Telemetry"]
        Auth["Authentication Events"]
        Billing["Billing Events"]
        Recommendation["Recommendation Engine"]
    end
    
    subgraph Events["Event Types"]
        E1["user.login"]
        E2["video.play"]
        E3["video.pause"]
        E4["subscription.renewed"]
        E5["profile.created"]
    end
    
    UI --> E1
    UI --> E5
    Playback --> E2
    Playback --> E3
    Billing --> E4
```

**VÃ­ dá»¥ Event Flow:**
1. User Ä‘Äƒng nháº­p vÃ o Netflix app
2. Action Ä‘Æ°á»£c gá»­i Ä‘áº¿n API Gateway
3. Gateway publish event `user.logged_in` vÃ o Kafka
4. CÃ¡c downstream services consume event nÃ y

### 2.2 Event Schema

Netflix sá»­ dá»¥ng má»™t schema chuáº©n cho events:

```json
{
  "event_id": "uuid-v4",
  "event_type": "video.play.started",
  "timestamp": "2024-12-26T12:00:00Z",
  "version": "1.0",
  "source": {
    "service": "playback-service",
    "device_type": "smart_tv",
    "region": "us-east-1"
  },
  "payload": {
    "user_id": "user-123",
    "video_id": "stranger-things-s4e01",
    "profile_id": "profile-456",
    "quality": "4K",
    "position_ms": 0
  },
  "metadata": {
    "trace_id": "trace-789",
    "correlation_id": "corr-101"
  }
}
```

### 2.3 Event Categories

| Category | VÃ­ dá»¥ Events | Volume |
|----------|--------------|--------|
| **Playback Events** | play, pause, seek, stop | Ráº¥t cao |
| **User Events** | login, logout, profile_switch | Cao |
| **Billing Events** | subscription_created, payment_processed | Trung bÃ¬nh |
| **Content Events** | content_added, metadata_updated | Tháº¥p |
| **System Events** | health_check, error_logged | Cao |

---

## 3. Keystone Pipeline

### 3.1 Tá»•ng Quan Keystone

**Keystone Pipeline** lÃ  háº¡ táº§ng thá»‘ng nháº¥t cá»§a Netflix cho viá»‡c publishing, collecting, vÃ  routing events. ÄÃ¢y lÃ  má»™t trong nhá»¯ng thÃ nh pháº§n quan trá»ng nháº¥t trong EDA cá»§a Netflix.

```mermaid
flowchart TB
    subgraph Sources["ğŸ“¤ Event Sources"]
        S1["Microservices"]
        S2["Client Apps"]
        S3["Backend Systems"]
    end
    
    subgraph Keystone["ğŸ”‘ Keystone Pipeline"]
        direction TB
        Router["Event Router"]
        Buffer["Kafka Buffer"]
        Transform["Stream Transformers"]
        
        Router --> Buffer
        Buffer --> Transform
    end
    
    subgraph Sinks["ğŸ“¥ Event Sinks"]
        RT["Real-time Processing"]
        Batch["Batch Processing"]
        Archive["Data Archive"]
    end
    
    Sources --> Router
    Transform --> Sinks
    
    style Keystone fill:#667eea,color:#fff
```

### 3.2 Kiáº¿n TrÃºc Keystone Chi Tiáº¿t

**CÃ¡c thÃ nh pháº§n:**

1. **Event Publisher SDK**
   - Cung cáº¥p cho cÃ¡c service Ä‘á»ƒ publish events
   - Há»— trá»£ batching vÃ  compression
   - Automatic retry vá»›i exponential backoff

2. **Kafka Clusters**
   - Multiple clusters cho high availability
   - Topic-based routing
   - Multi-region replication

3. **Router Service**
   - Rule-based routing
   - Content-based filtering
   - Dynamic topology

4. **Stream Processors**
   - Flink jobs cho real-time processing
   - Spark jobs cho batch processing

### 3.3 Data Flow trong Keystone

```mermaid
sequenceDiagram
    participant Service as Microservice
    participant SDK as Publisher SDK
    participant Kafka as Kafka Cluster
    participant Router as Event Router
    participant Flink as Flink Job
    participant Storage as Data Storage
    
    Service->>SDK: publish(event)
    SDK->>SDK: batch & compress
    SDK->>Kafka: produce(batch)
    Kafka->>Router: consume(batch)
    Router->>Router: apply routing rules
    Router->>Kafka: produce to target topics
    Kafka->>Flink: consume(events)
    Flink->>Flink: process & transform
    Flink->>Storage: persist results
```

### 3.4 Giáº£i ThÃ­ch Chi Tiáº¿t Data Flow trong Keystone

Data flow trong Keystone Ä‘Æ°á»£c chia thÃ nh **3 phases chÃ­nh**, má»—i phase cÃ³ nhiá»‡m vá»¥ vÃ  Ä‘áº·c Ä‘iá»ƒm riÃªng:

#### ğŸ“¤ **Phase 1: Event Publishing (Producer Side)**

```mermaid
flowchart LR
    subgraph App["Application Layer"]
        Code["Application Code"]
        Event["Event Object"]
    end
    
    subgraph SDK["Publisher SDK"]
        Validate["Schema Validation"]
        Batch["Batching"]
        Compress["Compression"]
        Retry["Retry Logic"]
    end
    
    subgraph Kafka["Kafka Frontend"]
        Topic["Fronting Topic"]
    end
    
    Code --> Event --> Validate --> Batch --> Compress --> Retry --> Topic
    
    style SDK fill:#3498db,color:#fff
```

**BÆ°á»›c 1: Service gá»i `publish(event)` thÃ´ng qua SDK**

```java
// VÃ­ dá»¥: Playback service publish event khi user báº¯t Ä‘áº§u xem video
KeystoneEvent event = KeystoneEvent.builder()
    .eventType("video.play.started")
    .userId("user-123")
    .videoId("stranger-things-s4e01")
    .timestamp(Instant.now())
    .deviceInfo(deviceInfo)
    .build();

keystonePublisher.publish(event);
```

| Thao tÃ¡c | MÃ´ táº£ chi tiáº¿t |
|----------|----------------|
| **Create Event** | Service táº¡o event object vá»›i Ä‘áº§y Ä‘á»§ fields theo schema |
| **Call SDK** | SDK nháº­n event thÃ´ng qua `publish()` method |
| **Validate** | SDK validate event against registered Avro/Protobuf schema |

**BÆ°á»›c 2: SDK thá»±c hiá»‡n Batching & Compression**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Publisher SDK Processing                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Event 1 â”€â”€â”                                                â”‚
â”‚  Event 2 â”€â”€â”¼â”€â”€â–¶ [Batch Buffer] â”€â”€â–¶ [LZ4 Compress] â”€â”€â–¶ Kafka â”‚
â”‚  Event 3 â”€â”€â”˜         â†‘                                      â”‚
â”‚                      â”‚                                       â”‚
â”‚              Flush triggers:                                 â”‚
â”‚              â€¢ Buffer full (1000 events)                    â”‚
â”‚              â€¢ Timeout (100ms)                              â”‚
â”‚              â€¢ Manual flush                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Ká»¹ thuáº­t | Chi tiáº¿t | Lá»£i Ã­ch |
|----------|----------|---------|
| **Batching** | Gom nhiá»u events thÃ nh 1 batch (max 1000 events hoáº·c 100ms) | Giáº£m network overhead, tÄƒng throughput |
| **Compression** | Sá»­ dá»¥ng LZ4 compression algorithm | Giáº£m 60-70% kÃ­ch thÆ°á»›c, tá»‘i Æ°u bandwidth |
| **Async I/O** | Non-blocking writes vá»›i internal queue | KhÃ´ng block application thread |

**BÆ°á»›c 3: SDK produce batch vÃ o Kafka**

```java
// Internal SDK logic (simplified)
public class KeystoneProducer {
    private final BlockingQueue<Event> buffer;
    private final KafkaProducer<String, byte[]> producer;
    
    public void publish(Event event) {
        // Add to buffer (non-blocking)
        buffer.offer(event);
    }
    
    // Background thread
    private void flushBuffer() {
        List<Event> batch = drainBuffer();
        byte[] compressed = compress(serialize(batch));
        
        ProducerRecord<String, byte[]> record = new ProducerRecord<>(
            "keystone-fronting-topic",
            event.getUserId(),  // Partition key
            compressed
        );
        
        producer.send(record, (metadata, exception) -> {
            if (exception != null) {
                retryWithBackoff(batch);
            }
        });
    }
}
```

**Error Handling trong Phase 1:**

| Error Type | Handling Strategy |
|------------|-------------------|
| Schema validation failed | Reject event, log error, increment metric |
| Kafka unavailable | Retry vá»›i exponential backoff (1s, 2s, 4s...) |
| Buffer overflow | Apply backpressure, block producer temporarily |
| Network timeout | Retry up to 3 times, then send to DLQ |

---

#### ğŸ“¨ **Phase 2: Routing & Distribution (Keystone Core)**

```mermaid
flowchart TB
    subgraph Input["Fronting Topics"]
        FT1["fronting-topic-1"]
        FT2["fronting-topic-2"]
    end
    
    subgraph Router["Event Router Service"]
        Consume["Consumer Group"]
        Decompress["Decompress"]
        Parse["Parse Events"]
        Rules["Apply Routing Rules"]
        Enrich["Enrich Metadata"]
    end
    
    subgraph Output["Target Topics"]
        T1["analytics-events"]
        T2["realtime-events"]
        T3["archive-events"]
        T4["ml-training-events"]
    end
    
    Input --> Consume --> Decompress --> Parse --> Rules --> Enrich
    Enrich --> T1
    Enrich --> T2
    Enrich --> T3
    Enrich --> T4
    
    style Router fill:#9b59b6,color:#fff
```

**BÆ°á»›c 4: Router consume tá»« Kafka Fronting Topics**

Router Service lÃ  má»™t cluster cá»§a cÃ¡c Flink/custom Java applications cháº¡y nhÆ° consumer group:

```yaml
# Router Configuration
router:
  consumer:
    group-id: "keystone-router"
    topics:
      - "keystone-fronting-*"  # Wildcard pattern
    parallelism: 100           # 100 consumer instances
    poll-timeout: 100ms
    
  processing:
    decompression: LZ4
    deserialization: AVRO
    max-batch-size: 5000
```

**BÆ°á»›c 5: Apply Routing Rules**

Routing rules Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a báº±ng DSL vÃ  stored trong configuration service:

```yaml
# Routing Rules Configuration
routing-rules:
  - name: "playback-to-analytics"
    condition:
      event-type: "video.play.*"
    destinations:
      - topic: "analytics-playback-events"
        priority: HIGH
      - topic: "ml-recommendation-input"
        priority: NORMAL
        
  - name: "billing-events"
    condition:
      event-type: "billing.*"
      region: "us-*"
    destinations:
      - topic: "billing-events-us"
        transform:
          - mask-pii-fields
          
  - name: "all-events-archive"
    condition: "*"  # Match all
    destinations:
      - topic: "archive-all-events"
        priority: LOW
```

**Routing Decision Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Routing Decision Engine                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Event                                                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Match Conditions â”‚â—€â”€â”€â”€ Event Type, Region, User Segment     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Resolve Topics  â”‚â—€â”€â”€â”€ Static or Dynamic topic resolution    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Apply Transform â”‚â—€â”€â”€â”€ PII masking, enrichment, filtering   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Fan-out to N    â”‚â—€â”€â”€â”€ One event â†’ Multiple destinations    â”‚
â”‚  â”‚ Destinations    â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BÆ°á»›c 6: Produce to Target Topics**

| Destination Type | Kafka Cluster | Retention | Use Case |
|------------------|---------------|-----------|----------|
| **Real-time topics** | Hot Cluster | 2 hours | Flink streaming jobs |
| **Analytics topics** | Warm Cluster | 7 days | Spark batch jobs |
| **Archive topics** | Cold Cluster | 30 days | S3 archival |
| **ML topics** | ML Cluster | 14 days | Model training |

---

#### âš¡ **Phase 3: Processing & Delivery (Consumer Side)**

```mermaid
flowchart TB
    subgraph Kafka["Target Kafka Topics"]
        Topic["analytics-events"]
    end
    
    subgraph Flink["Flink Processing"]
        Source["Kafka Source"]
        Deserialize["Deserialize"]
        Transform["Transform & Enrich"]
        Window["Windowing"]
        Aggregate["Aggregation"]
    end
    
    subgraph Sinks["Data Sinks"]
        Cassandra["Cassandra<br/>(Real-time queries)"]
        S3["S3 / Iceberg<br/>(Data Lake)"]
        EVCache["EVCache<br/>(Hot data)"]
        Druid["Druid<br/>(OLAP)"]
    end
    
    Kafka --> Source --> Deserialize --> Transform --> Window --> Aggregate
    Aggregate --> Cassandra
    Aggregate --> S3
    Aggregate --> EVCache
    Aggregate --> Druid
    
    style Flink fill:#e74c3c,color:#fff
```

**BÆ°á»›c 7 & 8: Flink consume vÃ  process events**

```java
// Flink Job Example: User Activity Aggregation
public class UserActivityAggregationJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Configure checkpointing for exactly-once
        env.enableCheckpointing(60000); // Every 1 minute
        env.setStateBackend(new RocksDBStateBackend("s3://checkpoints/"));
        
        // Step 7: Consume from Kafka
        KafkaSource<PlaybackEvent> source = KafkaSource
            .<PlaybackEvent>builder()
            .setBootstrapServers("kafka-cluster:9092")
            .setTopics("analytics-playback-events")
            .setGroupId("user-activity-aggregation")
            .setStartingOffsets(OffsetsInitializer.committedOffsets())
            .setDeserializer(new PlaybackEventDeserializer())
            .build();
        
        DataStream<PlaybackEvent> events = env.fromSource(
            source, 
            WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(5)),
            "Playback Events"
        );
        
        // Step 8: Process & Transform
        DataStream<UserActivityStats> stats = events
            // Filter valid events
            .filter(e -> e.getUserId() != null && e.getVideoId() != null)
            // Key by user
            .keyBy(PlaybackEvent::getUserId)
            // 5-minute tumbling windows
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            // Aggregate stats
            .aggregate(new UserActivityAggregator());
        
        // Step 9: Write to multiple sinks
        stats.addSink(new CassandraSink<>()); // Real-time queries
        stats.addSink(new IcebergSink<>());   // Data lake
        
        env.execute("User Activity Aggregation");
    }
}
```

**BÆ°á»›c 9: Persist Results**

| Storage | Data Type | Access Pattern | Latency |
|---------|-----------|----------------|---------|
| **EVCache** | Hot aggregates (last 1 hour) | Key-value lookup | < 1ms |
| **Cassandra** | User activity history | Time-series queries | < 10ms |
| **Iceberg/S3** | Raw events + aggregates | Batch analytics | Minutes |
| **Druid** | OLAP cubes | Interactive dashboards | < 1s |

---

### 3.5 End-to-End Example: User xem Stranger Things

Äá»ƒ hiá»ƒu rÃµ hÆ¡n vá» data flow, hÃ£y xem má»™t vÃ­ dá»¥ cá»¥ thá»ƒ:

```mermaid
sequenceDiagram
    autonumber
    actor User
    participant App as Netflix App
    participant Playback as Playback Service
    participant SDK as Keystone SDK
    participant Kafka1 as Kafka (Fronting)
    participant Router as Router Service
    participant Kafka2 as Kafka (Target)
    participant Flink as Flink Job
    participant DB as Cassandra
    participant Rec as Recommendation Engine
    
    User->>App: Click "Play" on Stranger Things S4E01
    App->>Playback: startPlayback(videoId, userId)
    Playback->>SDK: publish(PlayStartedEvent)
    
    Note over SDK: Batch vá»›i cÃ¡c events khÃ¡c<br/>Compress báº±ng LZ4
    SDK->>Kafka1: produce("keystone-fronting", batch)
    
    Kafka1->>Router: consume(batch)
    Note over Router: Decompress<br/>Apply routing rules<br/>Fan-out to 3 topics
    
    Router->>Kafka2: produce("analytics-playback")
    Router->>Kafka2: produce("ml-recommendation")
    Router->>Kafka2: produce("archive-all")
    
    Kafka2->>Flink: consume("analytics-playback")
    Note over Flink: Aggregate user stats<br/>trong 5-min window
    
    Flink->>DB: write(UserActivityStats)
    Flink->>Rec: triggerRecommendationUpdate(userId)
    
    Note over Rec: Update recommendations<br/>for user
```

**Timeline cá»§a Event:**

| Time | Component | Action | Latency |
|------|-----------|--------|---------|
| T+0ms | Netflix App | User clicks Play | - |
| T+5ms | Playback Service | Calls SDK.publish() | 5ms |
| T+10ms | SDK | Adds to batch buffer | 5ms |
| T+100ms | SDK | Flushes batch to Kafka | 90ms (batch timeout) |
| T+150ms | Kafka Fronting | Acknowledges write | 50ms |
| T+200ms | Router | Consumes and routes | 50ms |
| T+250ms | Kafka Target | Events in target topics | 50ms |
| T+300ms | Flink | Consumes event | 50ms |
| T+5min | Flink | Window closes, aggregates | ~5 min (window) |
| T+5min+100ms | Cassandra | Stats persisted | 100ms |

**Tá»•ng end-to-end latency**: ~300ms cho event available trong target topic, ~5 phÃºt cho aggregated stats.

---

### 3.6 Error Handling vÃ  Recovery trong Keystone

```mermaid
flowchart TB
    subgraph Normal["Normal Flow"]
        E1["Event"] --> P1["Process"] --> S1["Success"]
    end
    
    subgraph Error["Error Handling"]
        E2["Event"] --> P2["Process"] --> F1["Failure"]
        F1 --> R1["Retry (3x)"]
        R1 -->|Success| S2["Success"]
        R1 -->|All Failed| DLQ["Dead Letter Queue"]
        DLQ --> Alert["Alert Team"]
        DLQ --> Manual["Manual Review"]
    end
    
    style DLQ fill:#e74c3c,color:#fff
    style Alert fill:#f39c12,color:#fff
```

| Error Scenario | Detection | Recovery |
|----------------|-----------|----------|
| **Schema mismatch** | SDK validation | Reject + log + alert |
| **Kafka unavailable** | Connection timeout | Retry with backoff |
| **Router crash** | Health check failure | Auto-restart + rebalance |
| **Flink job failure** | Checkpoint failure | Restore from checkpoint |
| **Poison message** | Processing exception | Send to DLQ |

---

## 4. Real-Time Distributed Graph (RDG)

### 4.1 Táº¡i sao cáº§n RDG?

Netflix cáº§n theo dÃµi vÃ  phÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i dÃ¹ng theo thá»i gian thá»±c Ä‘á»ƒ:
- Cung cáº¥p recommendations chÃ­nh xÃ¡c
- PhÃ¡t hiá»‡n fraud vÃ  anomalies
- Cáº£i thiá»‡n tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng

### 4.2 Kiáº¿n TrÃºc RDG

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input Layer"]
        Login["Login Event"]
        Watch["Watch Event"]
        Browse["Browse Event"]
    end
    
    subgraph Kafka["ğŸ“¨ Kafka Layer"]
        Topics["Kafka Topics"]
    end
    
    subgraph Flink["âš¡ Flink Processing"]
        Consume["Consumer"]
        Process["Processor Functions"]
        Enrich["Enrichment"]
    end
    
    subgraph Graph["ğŸ”— Graph Layer"]
        direction LR
        Nodes["Graph Nodes"]
        Edges["Graph Edges"]
        Queries["Graph Queries"]
    end
    
    Input --> Kafka
    Kafka --> Flink
    Flink --> Graph
    
    style Graph fill:#e056fd,color:#fff
```

### 4.3 Anatomy cá»§a má»™t RDG Flink Job

**VÃ­ dá»¥ flow khi user xem Stranger Things:**

```
1. User login vÃ o Netflix app
2. User báº¯t Ä‘áº§u xem Stranger Things S4E01
3. Events Ä‘Æ°á»£c write vÃ o Kafka topics
4. Flink job consume events
5. Processor functions:
   - Parse vÃ  validate event
   - Enrich vá»›i metadata
   - Update graph relationships
   - Emit processed events
```

**CÃ¡c Processor Functions:**

| Function | MÃ´ táº£ |
|----------|-------|
| **EventParser** | Parse raw events tá»« Kafka |
| **EventValidator** | Validate schema vÃ  data integrity |
| **EventEnricher** | ThÃªm context vÃ  metadata |
| **GraphUpdater** | Update nodes vÃ  edges trong graph |
| **EventEmitter** | Emit processed events downstream |

### 4.4 Graph Data Model

```mermaid
graph LR
    User((User)) -->|WATCHES| Video((Video))
    User -->|HAS| Profile((Profile))
    User -->|USES| Device((Device))
    Video -->|BELONGS_TO| Series((Series))
    Video -->|HAS_GENRE| Genre((Genre))
    Profile -->|PREFERS| Genre
    
    style User fill:#ff6b6b,color:#fff
    style Video fill:#4ecdc4,color:#fff
```

---

## 5. Kafka - Backbone cá»§a Event Streaming

### 5.1 Netflix vÃ  Kafka

Netflix sá»­ dá»¥ng **Apache Kafka** lÃ m backbone cho toÃ n bá»™ event streaming infrastructure. CÃ¡c Ä‘áº·c Ä‘iá»ƒm chÃ­nh:

- **High Throughput**: Xá»­ lÃ½ hÃ ng triá»‡u messages/giÃ¢y
- **Durability**: Dá»¯ liá»‡u Ä‘Æ°á»£c replicate across brokers
- **Scalability**: Dá»… dÃ ng scale horizontally
- **Replayability**: CÃ³ thá»ƒ replay events cho debugging

### 5.2 Kafka Architecture táº¡i Netflix

```mermaid
flowchart TB
    subgraph Producers["Event Producers"]
        P1["Service A"]
        P2["Service B"]
        P3["Service C"]
    end
    
    subgraph Kafka["Kafka Cluster"]
        subgraph Brokers["Brokers"]
            B1["Broker 1"]
            B2["Broker 2"]
            B3["Broker 3"]
        end
        
        subgraph Topics["Topics"]
            T1["user-events"]
            T2["playback-events"]
            T3["billing-events"]
        end
    end
    
    subgraph Consumers["Event Consumers"]
        C1["Analytics Service"]
        C2["Recommendation Engine"]
        C3["Fraud Detection"]
    end
    
    Producers --> Kafka
    Kafka --> Consumers
    
    style Kafka fill:#231f20,color:#fff
```

### 5.3 Topic Design Patterns

| Pattern | Use Case | VÃ­ dá»¥ |
|---------|----------|-------|
| **Domain-based** | Events theo business domain | `billing.events`, `playback.events` |
| **Action-based** | Events theo action type | `user.created`, `video.played` |
| **Region-based** | Events theo geographic region | `events.us-east`, `events.eu-west` |

### 5.4 Kafka Best Practices táº¡i Netflix

1. **Partitioning Strategy**
   - Partition by user_id cho user-related events
   - Ensures ordering per user
   
2. **Retention Policies**
   - Hot data: 7 days retention
   - Cold data: Archive to S3
   
3. **Replication**
   - Minimum replication factor: 3
   - ISR (In-Sync Replicas) >= 2

4. **Compression**
   - Sá»­ dá»¥ng LZ4 cho balance giá»¯a speed vÃ  compression ratio

---

## 6. Apache Flink - Real-Time Processing

### 6.1 Táº¡i sao Flink?

Netflix chá»n **Apache Flink** cho real-time processing vÃ¬:

- **True Streaming**: KhÃ´ng pháº£i micro-batching nhÆ° Spark Streaming
- **Exactly-once Semantics**: Äáº£m báº£o má»—i event Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng 1 láº§n
- **Low Latency**: Xá»­ lÃ½ events trong milliseconds
- **Stateful Processing**: Quáº£n lÃ½ state hiá»‡u quáº£
- **Strong Internal Support**: Netflix cÃ³ platform support tá»‘t cho Flink

### 6.2 Flink Job Architecture

```mermaid
flowchart LR
    subgraph Source["ğŸ“¥ Source"]
        Kafka["Kafka Source"]
    end
    
    subgraph Operators["âš™ï¸ Operators"]
        Map["Map"]
        Filter["Filter"]
        KeyBy["KeyBy"]
        Window["Window"]
        Aggregate["Aggregate"]
    end
    
    subgraph Sink["ğŸ“¤ Sink"]
        DB["Database"]
        Cache["Cache"]
        KafkaOut["Kafka"]
    end
    
    Source --> Map --> Filter --> KeyBy --> Window --> Aggregate --> Sink
    
    style Operators fill:#1e88e5,color:#fff
```

### 6.3 VÃ­ dá»¥ Flink Job - User Activity Tracking

```java
// Pseudo-code minh há»a Flink job
public class UserActivityJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 1. Source: Consume tá»« Kafka
        DataStream<Event> events = env
            .addSource(new FlinkKafkaConsumer<>(
                "user-events",
                new EventSchema(),
                kafkaProps
            ));
        
        // 2. Process: Transform vÃ  enrich
        DataStream<EnrichedEvent> enrichedEvents = events
            .filter(e -> e.getType().equals("video.play"))
            .keyBy(Event::getUserId)
            .process(new UserEnrichmentFunction());
        
        // 3. Window: Aggregate theo thá»i gian
        DataStream<UserStats> stats = enrichedEvents
            .keyBy(EnrichedEvent::getUserId)
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            .aggregate(new UserStatsAggregator());
        
        // 4. Sink: Write káº¿t quáº£
        stats.addSink(new CassandraSink<>());
        
        env.execute("User Activity Tracking");
    }
}
```

### 6.4 State Management

Netflix sá»­ dá»¥ng Flink's stateful processing Ä‘á»ƒ:

- Track user sessions
- Maintain running aggregates
- Implement complex event processing

```mermaid
flowchart TB
    subgraph Job["Flink Job"]
        Op1["Operator 1"]
        Op2["Operator 2"]
        State["State Backend"]
        
        Op1 --> State
        Op2 --> State
    end
    
    subgraph Checkpoint["Checkpointing"]
        CP["Checkpoint Storage<br/>(S3)"]
    end
    
    State -.->|periodic snapshot| CP
    CP -.->|recovery| State
    
    style State fill:#ff9f43,color:#fff
```

---

## 7. Ads Event Processing Pipeline

### 7.1 Tá»•ng Quan

Vá»›i viá»‡c Netflix triá»ƒn khai tier cÃ³ quáº£ng cÃ¡o, há» Ä‘Ã£ xÃ¢y dá»±ng má»™t **Ads Event Processing Pipeline** chuyÃªn biá»‡t.

### 7.2 Kiáº¿n TrÃºc Pipeline

```mermaid
flowchart TB
    subgraph Client["ğŸ“± Client"]
        AdSDK["Ad SDK"]
    end
    
    subgraph Gateway["ğŸŒ Gateway"]
        AdGateway["Ad Event Gateway"]
    end
    
    subgraph Kafka["ğŸ“¨ Kafka"]
        AdTopics["Ad Event Topics"]
    end
    
    subgraph Processing["âš¡ Processing"]
        Validator["Event Validator"]
        Enricher["Event Enricher"]
        Router["Event Router"]
    end
    
    subgraph Sinks["ğŸ“¤ Sinks"]
        AdServer["Ad Server"]
        Analytics["Ad Analytics"]
        Billing["Ad Billing"]
    end
    
    Client --> Gateway --> Kafka --> Processing --> Sinks
    
    style Processing fill:#9b59b6,color:#fff
```

### 7.3 Ad Event Types

| Event Type | MÃ´ táº£ | Latency Requirement |
|------------|-------|---------------------|
| `ad.impression` | Quáº£ng cÃ¡o Ä‘Æ°á»£c hiá»ƒn thá»‹ | < 100ms |
| `ad.click` | User click vÃ o quáº£ng cÃ¡o | < 50ms |
| `ad.complete` | Quáº£ng cÃ¡o xem háº¿t | < 200ms |
| `ad.skip` | User skip quáº£ng cÃ¡o | < 100ms |

### 7.4 Xá»­ LÃ½ Real-time

Pipeline Ä‘áº£m báº£o:
- **Low Latency**: Events Ä‘Æ°á»£c xá»­ lÃ½ trong < 1 giÃ¢y
- **Exactly-once Delivery**: Má»—i ad impression Ä‘Æ°á»£c count chÃ­nh xÃ¡c 1 láº§n
- **Fraud Detection**: PhÃ¡t hiá»‡n click fraud real-time

---

## 8. Best Practices vÃ  Lessons Learned

### 8.1 Event Design Principles

1. **Events nÃªn Immutable**
   - Má»™t khi event Ä‘Æ°á»£c publish, khÃ´ng thá»ƒ thay Ä‘á»•i
   - Sá»­ dá»¥ng versioning cho schema changes

2. **Include Enough Context**
   - Event nÃªn self-contained
   - TrÃ¡nh viá»‡c consumer pháº£i lookup thÃªm data

3. **Use Standard Schemas**
   - Avro hoáº·c Protobuf cho serialization
   - Schema Registry cho version management

### 8.2 Scalability Patterns

```mermaid
flowchart LR
    subgraph Pattern1["Horizontal Scaling"]
        P1["Add more<br/>Kafka brokers"]
        P2["Add more<br/>Flink slots"]
        P3["Increase<br/>partitions"]
    end
    
    subgraph Pattern2["Data Partitioning"]
        P4["Partition by<br/>user_id"]
        P5["Partition by<br/>region"]
        P6["Partition by<br/>time"]
    end
    
    subgraph Pattern3["Backpressure"]
        P7["Rate limiting"]
        P8["Queue buffering"]
        P9["Graceful degradation"]
    end
```

### 8.3 Reliability Patterns

| Pattern | MÃ´ táº£ | Implementation |
|---------|-------|----------------|
| **Retry with Backoff** | Retry failed operations | Exponential backoff (1s, 2s, 4s, 8s...) |
| **Dead Letter Queue** | Handle poison messages | Separate topic cho failed events |
| **Circuit Breaker** | Prevent cascade failures | Hystrix (Ä‘Ã£ deprecated) â†’ Resilience4j |
| **Idempotency** | Handle duplicate events | Event ID deduplication |

### 8.4 Monitoring vÃ  Observability

Netflix sá»­ dá»¥ng:

- **Atlas**: Time-series metrics database (Netflix OSS)
- **Mantis**: Real-time stream processing cho alerting
- **Spectator**: Client library cho metrics collection

**Key Metrics:**

| Metric | MÃ´ táº£ | Alert Threshold |
|--------|-------|-----------------|
| Event Lag | Äá»™ trá»… giá»¯a produce vÃ  consume | > 5 phÃºt |
| Processing Latency | Thá»i gian xá»­ lÃ½ 1 event | P99 > 1s |
| Error Rate | % events failed | > 0.1% |
| Throughput | Events/second | < expected baseline |

---

## 9. So SÃ¡nh vá»›i CÃ¡c Giáº£i PhÃ¡p KhÃ¡c

### 9.1 Netflix vs. Uber

| Aspect | Netflix | Uber |
|--------|---------|------|
| Primary Use Case | Content streaming & recommendations | Real-time ride matching |
| Message Broker | Kafka | Kafka + Pub/Sub |
| Processing Engine | Flink | Flink + Samza |
| Scale | ~10M events/sec | ~100M events/sec |

### 9.2 Netflix vs. LinkedIn

| Aspect | Netflix | LinkedIn |
|--------|---------|----------|
| Kafka Role | Event streaming | Event + Log aggregation |
| Custom Components | Keystone, Mantis | Brooklin, Gobblin |
| Data Lake | S3 + Iceberg | HDFS + Pinot |

### 9.3 EDA vs. Traditional Architecture

```mermaid
flowchart TB
    subgraph Traditional["Traditional (Request-Response)"]
        A1[Service A] -->|sync call| B1[Service B]
        B1 -->|sync call| C1[Service C]
    end
    
    subgraph EDA["Event-Driven"]
        A2[Service A] -->|publish| Event[Event Bus]
        Event -->|subscribe| B2[Service B]
        Event -->|subscribe| C2[Service C]
    end
    
    style Event fill:#e74c3c,color:#fff
```

| Aspect | Traditional | Event-Driven |
|--------|-------------|--------------|
| Coupling | Tight | Loose |
| Scalability | Limited | High |
| Latency | Lower (sync) | Higher (async) |
| Complexity | Lower | Higher |
| Debugging | Easier | More challenging |

---

## 10. Káº¿t Luáº­n

### 10.1 Key Takeaways

1. **Kafka lÃ  trÃ¡i tim cá»§a Netflix EDA** - Táº¥t cáº£ events flow qua Kafka
2. **Flink cho real-time processing** - Xá»­ lÃ½ events vá»›i latency tháº¥p
3. **Keystone unifies everything** - Cung cáº¥p abstraction layer cho event publishing
4. **Schema evolution quan trá»ng** - Sá»­ dá»¥ng versioned schemas
5. **Monitoring lÃ  critical** - KhÃ´ng thá»ƒ manage cÃ¡i gÃ¬ khÃ´ng Ä‘o Ä‘Æ°á»£c

### 10.2 Khi nÃ o nÃªn Ã¡p dá»¥ng EDA?

âœ… **NÃªn dÃ¹ng khi:**
- Há»‡ thá»‘ng cáº§n scale lá»›n
- Cáº§n real-time processing
- Nhiá»u consumers cho cÃ¹ng má»™t event
- Cáº§n audit trail

âŒ **KhÃ´ng nÃªn dÃ¹ng khi:**
- Há»‡ thá»‘ng Ä‘Æ¡n giáº£n
- Cáº§n response Ä‘á»“ng bá»™
- Team chÆ°a cÃ³ experience vá»›i EDA
- Infrastructure háº¡n cháº¿

### 10.3 Learning Path

```mermaid
flowchart LR
    A[Learn Kafka Basics] --> B[Understand Event Design]
    B --> C[Practice with Flink/Kafka Streams]
    C --> D[Implement Patterns]
    D --> E[Scale & Monitor]
    
    style A fill:#3498db,color:#fff
    style E fill:#27ae60,color:#fff
```

---

## Tham Kháº£o

1. [Netflix Tech Blog - How and Why Netflix Built a Real-Time Distributed Graph](https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc)
2. [Netflix Tech Blog - Kafka Inside Keystone Pipeline](https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb)
3. [Netflix Tech Blog - Evolution of the Netflix Data Pipeline](https://netflixtechblog.com/evolution-of-the-netflix-data-pipeline-da246ca36905)
4. [Netflix Tech Blog - Behind the Scenes: Building a Robust Ads Event Processing Pipeline](https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249)
5. [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
6. [Apache Flink Documentation](https://flink.apache.org/docs/stable/)

---

*TÃ i liá»‡u Ä‘Æ°á»£c táº¡o: 2024-12-26*
*Cáº­p nháº­t láº§n cuá»‘i: 2024-12-26*
